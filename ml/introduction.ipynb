{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4OuZRSicXQg"
      },
      "source": [
        "# Introduction to Machine Learning\n",
        "\n",
        "There are probably a thousand articles called *introduction to machine learning*. Rather than rewrite this, I will instead introduce the main ideas focused on a chemistry example. Here are some introductory sources, and please do recommend new ones to me:\n",
        "\n",
        "1. The book I first read in grad school about machine learning by Ethem Alpaydin{cite}`alpaydin2020introduction`\n",
        "1. Nils Nillson's online book [<ins>Introductory Machine Learning</ins>](https://ai.stanford.edu/~nilsson/mlbook.html)\n",
        "2. Two reviews of machine learning in materials{cite}`fung2021benchmarking,balachandran2019machine`\n",
        "3. A review of machine learning in computational chemistry{cite}`gomez2020machine`\n",
        "4. A review of machine learning in metals{cite}`nandy2018strategies`\n",
        "\n",
        "I hope you learn from these sources about how machine learning is a method of modeling data, typically with predictive functions. Machine learning includes many techniques, but here we will focus on only those necessary to transition into deep learning. For example, random forests, support vector machines, and nearest neighbor are widely-used machine learning techniques that are effective but not covered here.\n",
        "\n",
        "```{admonition} Audience & Objectives\n",
        "This chapter is intended for novices of machine learning with familiarity of chemistry and python. It is recommended that you look over one of the above recommended introductory articles. This specific article assumes a very small amount of knowledge of the `pandas` library (loading and selecting a column), awareness of `rdkit` (how we draw molecules), and that we store/retrieve molecules as [SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) {cite}`weininger1988smiles`. After reading this, you should be able to:\n",
        "\n",
        "  * Define features, labels\n",
        "  * Distinguish between supervised and unsupervised learning\n",
        "  * Understand what a loss function is and how it can be minimized with gradient descent\n",
        "  * Understand what model is and its connection to features and labels\n",
        "  * Be able to cluster data and describe what it tells us about data\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sClAUfuKcXQk"
      },
      "source": [
        "## The Ingredients \n",
        "\n",
        "Machine learning the fitting of models $\\hat{f}(\\vec{x})$ to data $\\vec{x}, y$ that we know came from some ``data generation'' process $f(x)$ . Firstly, definitions:\n",
        "\n",
        "**Features** \n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;set of $N$ vectors $\\{\\vec{x}_i\\}$ of dimension $D$. Can be reals, integers, etc.\n",
        "\n",
        "**Labels** \n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;set of $N$ integers or reals $\\{y_i\\}$. $y_i$ is usually a scalar\n",
        "  \n",
        "**Labeled Data** \n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;set of $N$ tuples $\\{\\left(\\vec{x}_i, y_i\\right)\\}$ \n",
        "\n",
        "**Unlabeled Data** \n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;set of $N$ features  $\\{\\vec{x}_i\\}$  that may have unknown $y$ labels\n",
        "\n",
        "**Data generation process**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;The unseen process $f(\\vec{x})$ that takes a given feature vector in and returns a real label $y$ (what we're trying to model)\n",
        "\n",
        "**Model**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;A function $\\hat{f}(\\vec{x})$ that takes a given feature vector in and returns a predicted $\\hat{y}$\n",
        "\n",
        "**Predictions**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $\\hat{y}$, our predicted output for a given input $\\vec{x}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKpVFNGxcXQl"
      },
      "source": [
        "## Supervised Learning\n",
        "\n",
        "Our first task will be **supervised learning**. Supervised learning means predicting $y$ from $\\vec{x}$ with a model trained on data. It is *supervised* because we tell the algorithm what the labels are in our dataset. Another method we'll explore is **unsupervised learning** where we do not tell the algorithm the labels. We'll see this supervised/unsupervised distinction can be more subtle later on, but this is a great definition for now. \n",
        "\n",
        "To see an example, we will use a dataset called AqSolDB{cite}`Sorkun2019` that is about 10,000 unique compounds with measured solubility in water (label). The dataset also includes molecular properties (features) that we can use for machine learning. The solubility measurement is solubility of the compound in water in units of log molarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H36rdp56cXQm"
      },
      "source": [
        "## Running This Notebook\n",
        "\n",
        "\n",
        "Click the &nbsp;<i aria-label=\"Launch interactive content\" class=\"fas fa-rocket\"></i>&nbsp; above to launch this page as an interactive Google Colab. See details below on installing packages.\n",
        "\n",
        "````{tip} My title\n",
        ":class: dropdown\n",
        "To install packages, execute this code in a new cell. \n",
        "\n",
        "```\n",
        "!pip install dmol-book\n",
        "```\n",
        "\n",
        "If you find install problems, you can get the latest working versions of packages used in [this book here](https://github.com/whitead/dmol-book/blob/main/package/setup.py)\n",
        "\n",
        "````"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dmol-book"
      ],
      "metadata": {
        "id": "K5vxLtMOcfEa",
        "outputId": "366549d4-5ba8-4180-8a0c-c876fe64656d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dmol-book\n",
            "  Downloading dmol_book-1.3.2-py3-none-any.whl (2.4 kB)\n",
            "Collecting jupyter-book==0.13.1\n",
            "  Downloading jupyter_book-0.13.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 KB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from dmol-book) (1.22.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from dmol-book) (0.8.10)\n",
            "Collecting torch==1.12\n",
            "  Downloading torch-1.12.0-cp39-cp39-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from dmol-book) (1.2.2)\n",
            "Collecting MDAnalysis\n",
            "  Downloading MDAnalysis-2.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from dmol-book) (1.4.4)\n",
            "Collecting selfies\n",
            "  Downloading selfies-2.1.1-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: tensorflow>=2.7 in /usr/local/lib/python3.9/dist-packages (from dmol-book) (2.11.0)\n",
            "Collecting rdkit>=2022\n",
            "  Downloading rdkit-2022.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: moviepy in /usr/local/lib/python3.9/dist-packages (from dmol-book) (1.0.3)\n",
            "Collecting exmol\n",
            "  Downloading exmol-3.0.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.9/dist-packages (from dmol-book) (0.12.2)\n",
            "Collecting dm-haiku\n",
            "  Downloading dm_haiku-0.0.9-py3-none-any.whl (352 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 KB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from dmol-book) (3.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from dmol-book) (1.11.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.9/dist-packages (from dmol-book) (8.4.0)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.9/dist-packages (from dmol-book) (0.19.0)\n",
            "Collecting tensorflowjs\n",
            "  Downloading tensorflowjs-4.3.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.1/85.1 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simpletransformers==0.63.9\n",
            "  Downloading simpletransformers-0.63.9-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.5/250.5 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting e3nn\n",
            "  Downloading e3nn-0.5.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jaxlib in /usr/local/lib/python3.9/dist-packages (from dmol-book) (0.4.6+cuda11.cudnn86)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.9/dist-packages (from dmol-book) (1.4.2)\n",
            "Collecting emlp==1.0.2\n",
            "  Downloading emlp-1.0.2-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting linkify-it-py\n",
            "  Downloading linkify_it_py-2.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from dmol-book) (3.7.1)\n",
            "Requirement already satisfied: ipython!=8.7.0jax in /usr/local/lib/python3.9/dist-packages (from dmol-book) (7.9.0)\n",
            "Collecting mordred[full]\n",
            "  Downloading mordred-1.2.0.tar.gz (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.8/128.8 KB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.9/dist-packages (from emlp==1.0.2->dmol-book) (0.1.4)\n",
            "Collecting plum-dispatch\n",
            "  Downloading plum_dispatch-2.0.1-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: tqdm>=4.38 in /usr/local/lib/python3.9/dist-packages (from emlp==1.0.2->dmol-book) (4.65.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.9/dist-packages (from emlp==1.0.2->dmol-book) (7.2.2)\n",
            "Collecting objax\n",
            "  Downloading objax-1.6.0.tar.gz (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (from emlp==1.0.2->dmol-book) (3.8.0)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: docutils<0.18,>=0.15 in /usr/local/lib/python3.9/dist-packages (from jupyter-book==0.13.1->dmol-book) (0.16)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.9/dist-packages (from jupyter-book==0.13.1->dmol-book) (3.1.2)\n",
            "Collecting sphinx<5,>=4\n",
            "  Downloading Sphinx-4.5.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx-multitoc-numbering~=0.1.3\n",
            "  Downloading sphinx_multitoc_numbering-0.1.3-py3-none-any.whl (4.6 kB)\n",
            "Collecting sphinx-comments\n",
            "  Downloading sphinx_comments-0.0.3-py3-none-any.whl (4.6 kB)\n",
            "Collecting linkify-it-py\n",
            "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
            "Collecting sphinxcontrib-bibtex<=2.5.0,>=2.2.0\n",
            "  Downloading sphinxcontrib_bibtex-2.5.0-py3-none-any.whl (39 kB)\n",
            "Collecting sphinx-copybutton\n",
            "  Downloading sphinx_copybutton-0.5.1-py3-none-any.whl (13 kB)\n",
            "Collecting myst-nb~=0.13.1\n",
            "  Downloading myst_nb-0.13.2-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx-external-toc~=0.2.3\n",
            "  Downloading sphinx_external_toc-0.2.4-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from jupyter-book==0.13.1->dmol-book) (6.0)\n",
            "Requirement already satisfied: click<9,>=7.1 in /usr/local/lib/python3.9/dist-packages (from jupyter-book==0.13.1->dmol-book) (8.1.3)\n",
            "Collecting sphinx-design~=0.1.0\n",
            "  Downloading sphinx_design-0.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx_book_theme~=0.3.2\n",
            "  Downloading sphinx_book_theme-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.7/345.7 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx-thebe~=0.1.1\n",
            "  Downloading sphinx_thebe-0.1.2-py3-none-any.whl (8.3 kB)\n",
            "Collecting sphinx_togglebutton\n",
            "  Downloading sphinx_togglebutton-0.3.2-py3-none-any.whl (8.2 kB)\n",
            "Collecting sphinx-jupyterbook-latex~=0.4.6\n",
            "  Downloading sphinx_jupyterbook_latex-0.4.7-py3-none-any.whl (13 kB)\n",
            "Collecting jsonschema<4\n",
            "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers>=4.6.0\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb>=0.10.32\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit\n",
            "  Downloading streamlit-1.20.0-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from simpletransformers==0.63.9->dmol-book) (1.10.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from simpletransformers==0.63.9->dmol-book) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from simpletransformers==0.63.9->dmol-book) (2.27.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/dist-packages (from simpletransformers==0.63.9->dmol-book) (2.11.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.12->dmol-book) (4.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython!=8.7.0jax->dmol-book) (2.0.10)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipython!=8.7.0jax->dmol-book) (5.7.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython!=8.7.0jax->dmol-book) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython!=8.7.0jax->dmol-book) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.9/dist-packages (from ipython!=8.7.0jax->dmol-book) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython!=8.7.0jax->dmol-book) (67.6.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython!=8.7.0jax->dmol-book) (2.14.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython!=8.7.0jax->dmol-book) (4.4.2)\n",
            "Collecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (2.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (1.16.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (1.15.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (2.11.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (23.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (23.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (3.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (0.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (15.0.6.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (0.31.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (3.19.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7->dmol-book) (1.51.3)\n",
            "Collecting jmp>=0.0.2\n",
            "  Downloading jmp-0.0.4-py3-none-any.whl (18 kB)\n",
            "Collecting opt-einsum-fx>=0.1.4\n",
            "  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n",
            "Collecting ratelimit\n",
            "  Downloading ratelimit-2.2.1.tar.gz (5.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.123-py3-none-any.whl (426 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.3/426.3 KB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting skunk>=0.4.0\n",
            "  Downloading skunk-1.2.0-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.9/dist-packages (from exmol->dmol-book) (5.12.0)\n",
            "Collecting synspace\n",
            "  Downloading synspace-0.2.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->dmol-book) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->dmol-book) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->dmol-book) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->dmol-book) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->dmol-book) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->dmol-book) (4.39.2)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.9/dist-packages (from MDAnalysis->dmol-book) (3.1.0)\n",
            "Collecting GridDataFormats>=0.4.0\n",
            "  Downloading GridDataFormats-1.0.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.9/dist-packages (from MDAnalysis->dmol-book) (1.1.1)\n",
            "Collecting mmtf-python>=1.0.0\n",
            "  Downloading mmtf_python-1.1.3-py2.py3-none-any.whl (25 kB)\n",
            "Collecting biopython>=1.80\n",
            "  Downloading biopython-1.81-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gsd>=1.9.3\n",
            "  Downloading gsd-2.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (397 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m398.0/398.0 KB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners\n",
            "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
            "Collecting networkx\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from moviepy->dmol-book) (0.4.8)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.9/dist-packages (from moviepy->dmol-book) (2.25.1)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.9/dist-packages (from moviepy->dmol-book) (0.1.10)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->dmol-book) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->dmol-book) (1.3.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability->dmol-book) (0.1.8)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability->dmol-book) (2.2.1)\n",
            "Collecting tensorflow-decision-forests>=1.0.1\n",
            "  Downloading tensorflow_decision_forests-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flax>=0.6.2 in /usr/local/lib/python3.9/dist-packages (from tensorflowjs->dmol-book) (0.6.7)\n",
            "Requirement already satisfied: jax>=0.3.16 in /usr/local/lib/python3.9/dist-packages (from tensorflowjs->dmol-book) (0.4.6)\n",
            "Collecting packaging\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-hub<0.13,>=0.7.0\n",
            "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.7->dmol-book) (0.40.0)\n",
            "Requirement already satisfied: orbax in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs->dmol-book) (0.1.6)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs->dmol-book) (1.0.5)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs->dmol-book) (13.3.2)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs->dmol-book) (0.1.33)\n",
            "Collecting mrcfile\n",
            "  Downloading mrcfile-1.4.3-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources->exmol->dmol-book) (3.15.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.10->ipython!=8.7.0jax->dmol-book) (0.8.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema<4->jupyter-book==0.13.1->dmol-book) (22.2.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema<4->jupyter-book==0.13.1->dmol-book) (0.19.3)\n",
            "Requirement already satisfied: nbformat~=5.0 in /usr/local/lib/python3.9/dist-packages (from myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (5.8.0)\n",
            "Collecting myst-parser~=0.15.2\n",
            "  Downloading myst_parser-0.15.2-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipywidgets<8,>=7.0.0 in /usr/local/lib/python3.9/dist-packages (from myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (7.7.1)\n",
            "Collecting jupyter-cache~=0.4.1\n",
            "  Downloading jupyter_cache-0.4.3-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: nbconvert<7,>=5.6 in /usr/local/lib/python3.9/dist-packages (from myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (6.5.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.9/dist-packages (from myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (6.1.0)\n",
            "Collecting jupyter-sphinx~=0.3.2\n",
            "  Downloading jupyter_sphinx-0.3.2-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython!=8.7.0jax->dmol-book) (0.2.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->simpletransformers==0.63.9->dmol-book) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->simpletransformers==0.63.9->dmol-book) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->simpletransformers==0.63.9->dmol-book) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->simpletransformers==0.63.9->dmol-book) (3.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from sphinx<5,>=4->jupyter-book==0.13.1->dmol-book) (1.1.5)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.9/dist-packages (from sphinx<5,>=4->jupyter-book==0.13.1->dmol-book) (0.7.13)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.9/dist-packages (from sphinx<5,>=4->jupyter-book==0.13.1->dmol-book) (1.0.2)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.9/dist-packages (from sphinx<5,>=4->jupyter-book==0.13.1->dmol-book) (1.4.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.9/dist-packages (from sphinx<5,>=4->jupyter-book==0.13.1->dmol-book) (1.0.3)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.9/dist-packages (from sphinx<5,>=4->jupyter-book==0.13.1->dmol-book) (1.0.4)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.9/dist-packages (from sphinx<5,>=4->jupyter-book==0.13.1->dmol-book) (2.2.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.9/dist-packages (from sphinx<5,>=4->jupyter-book==0.13.1->dmol-book) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from sphinx<5,>=4->jupyter-book==0.13.1->dmol-book) (2.0.1)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.9/dist-packages (from sphinx<5,>=4->jupyter-book==0.13.1->dmol-book) (2.12.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2->jupyter-book==0.13.1->dmol-book) (2.1.2)\n",
            "Collecting pydata-sphinx-theme~=0.8.0\n",
            "  Downloading pydata_sphinx_theme-0.8.1-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs>=17.4.0\n",
            "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybtex-docutils>=1.0.0\n",
            "  Downloading pybtex_docutils-1.0.2-py3-none-any.whl (6.3 kB)\n",
            "Collecting pybtex>=0.24\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.4/561.4 KB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers==0.63.9->dmol-book) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers==0.63.9->dmol-book) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers==0.63.9->dmol-book) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers==0.63.9->dmol-book) (2.16.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers==0.63.9->dmol-book) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers==0.63.9->dmol-book) (1.8.1)\n",
            "Collecting tensorflow>=2.7\n",
            "  Downloading tensorflow-2.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wurlitzer\n",
            "  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n",
            "INFO: pip is looking at multiple versions of tensorflow-decision-forests to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-decision-forests>=1.0.1\n",
            "  Downloading tensorflow_decision_forests-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->simpletransformers==0.63.9->dmol-book) (3.10.2)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers==0.63.9->dmol-book) (5.9.4)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.18.0-py2.py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 KB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers==0.63.9->dmol-book) (1.4.4)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers==0.63.9->dmol-book) (2023.3.0)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers==0.63.9->dmol-book) (9.0.0)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain->exmol->dmol-book) (1.10.7)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain->exmol->dmol-book) (1.4.47)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain->exmol->dmol-book) (8.2.2)\n",
            "Collecting parameterized\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.9/dist-packages (from optax->emlp==1.0.2->dmol-book) (0.1.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect->ipython!=8.7.0jax->dmol-book) (0.7.0)\n",
            "Collecting beartype\n",
            "  Downloading beartype-0.12.0-py3-none-any.whl (754 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m754.5/754.5 KB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.9/dist-packages (from pytest->emlp==1.0.2->dmol-book) (1.0.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.9/dist-packages (from pytest->emlp==1.0.2->dmol-book) (2.0.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pytest->emlp==1.0.2->dmol-book) (2.0.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.9/dist-packages (from pytest->emlp==1.0.2->dmol-book) (1.1.1)\n",
            "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers==0.63.9->dmol-book) (6.2)\n",
            "Collecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers==0.63.9->dmol-book) (5.3.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers==0.63.9->dmol-book) (0.10.2)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 KB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blinker>=1.0.0\n",
            "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers==0.63.9->dmol-book) (4.3)\n",
            "Collecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: altair<5,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers==0.63.9->dmol-book) (4.2.2)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pystow\n",
            "  Downloading pystow-0.5.0-py3-none-any.whl (37 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit->simpletransformers==0.63.9->dmol-book) (0.12.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit->simpletransformers==0.63.9->dmol-book) (0.4)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect>=0.4.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers==0.63.9->dmol-book) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers==0.63.9->dmol-book) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers==0.63.9->dmol-book) (1.3.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (3.0.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (0.2.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (5.3.4)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (3.6.3)\n",
            "Collecting nbclient<0.6,>=0.2\n",
            "  Downloading nbclient-0.5.13-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nbdime\n",
            "  Downloading nbdime-3.1.1-py2.py3-none-any.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown-it-py<2.0.0,>=1.0.0\n",
            "  Downloading markdown_it_py-1.1.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdit-py-plugins~=0.2.8\n",
            "  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (1.2.1)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.9/dist-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (5.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (6.0.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (1.5.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (0.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (4.11.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (4.9.2)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat~=5.0->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (2.16.3)\n",
            "Collecting latexcodec>=1.0.4\n",
            "  Downloading latexcodec-2.0.1-py2.py3-none-any.whl (18 kB)\n",
            "Collecting rich>=11.1\n",
            "  Downloading rich-13.3.3-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.7/238.7 KB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading rich-13.3.1-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 KB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading rich-13.3.0-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 KB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading rich-13.2.0-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading rich-13.1.0-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.4/238.4 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain->exmol->dmol-book) (2.0.2)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.9/dist-packages (from tzlocal>=1.1->streamlit->simpletransformers==0.63.9->dmol-book) (0.1.0.post0)\n",
            "Requirement already satisfied: etils in /usr/local/lib/python3.9/dist-packages (from orbax->flax>=0.6.2->tensorflowjs->dmol-book) (1.1.1)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.9/dist-packages (from orbax->flax>=0.6.2->tensorflowjs->dmol-book) (1.5.6)\n",
            "Requirement already satisfied: cached_property in /usr/local/lib/python3.9/dist-packages (from orbax->flax>=0.6.2->tensorflowjs->dmol-book) (1.5.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (6.1.12)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.7->nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (3.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers==0.63.9->dmol-book) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers==0.63.9->dmol-book) (3.2.2)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (6.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (2.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (0.5.1)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting jupyter-server-mathjax>=0.2.2\n",
            "  Downloading jupyter_server_mathjax-0.2.6-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-server\n",
            "  Downloading jupyter_server-2.5.0-py3-none-any.whl (366 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.8/366.8 KB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tzdata in /usr/local/lib/python3.9/dist-packages (from pytz-deprecation-shim->tzlocal>=1.1->streamlit->simpletransformers==0.63.9->dmol-book) (2022.7)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.9/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (23.2.1)\n",
            "Collecting anyio>=3.1.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzmq>=13\n",
            "  Downloading pyzmq-25.0.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-client\n",
            "  Downloading jupyter_client-8.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.9/102.9 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websocket-client\n",
            "  Downloading websocket_client-1.5.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (0.17.1)\n",
            "Requirement already satisfied: send2trash in /usr/local/lib/python3.9/dist-packages (from jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (21.3.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (0.16.0)\n",
            "Collecting jupyter-events>=0.4.0\n",
            "  Downloading jupyter_events-0.6.3-py3-none-any.whl (18 kB)\n",
            "Collecting jupyter-server-terminals\n",
            "  Downloading jupyter_server_terminals-0.4.4-py3-none-any.whl (13 kB)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting rfc3339-validator\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: jsonschema[format-nongpl]>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-events>=0.4.0->jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (4.3.3)\n",
            "Collecting rfc3986-validator>=0.1.1\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting python-json-logger>=2.0.4\n",
            "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (21.2.0)\n",
            "\u001b[33mWARNING: jsonschema 4.3.3 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting jsonschema[format-nongpl]>=3.2.0\n",
            "  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.17.1-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.2/90.2 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.17.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.16.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.15.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.14.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.4/82.4 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.13.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.6/81.6 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.12.1-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.12.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.11.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.10.3-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.10.2-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.10.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.9.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.9.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.8.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.7.2-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.7.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.7.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.6.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.6.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.4/80.4 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading jsonschema-4.5.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jsonschema 4.5.1 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading jsonschema-4.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jsonschema 4.5.0 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading jsonschema-4.4.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jsonschema 4.4.0 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading jsonschema-4.3.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jsonschema 4.3.2 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading jsonschema-4.3.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/71.7 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jsonschema 4.3.1 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading jsonschema-4.3.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jsonschema 4.3.0 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading jsonschema-4.2.1-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.4/69.4 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jsonschema 4.2.1 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading jsonschema-4.2.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.4/69.4 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jsonschema 4.2.0 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading jsonschema-4.1.2-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jsonschema 4.1.2 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading jsonschema-4.1.1-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jsonschema 4.1.1 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading jsonschema-4.1.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jsonschema 4.1.0 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading jsonschema-4.0.1-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jsonschema 4.0.1 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: jsonschema 3.2.0 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book==0.13.1->dmol-book) (2.21)\n",
            "Building wheels for collected packages: mordred, objax, ratelimit, seqeval, sklearn, validators, pathtools\n",
            "  Building wheel for mordred (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mordred: filename=mordred-1.2.0-py3-none-any.whl size=176723 sha256=78d959beaab3a6609261c4381c0390df9c54950704feaf1876952b2119e8acc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/95/d1/9e913738f0e8362b3676917b953a60afd76d2b0b99ff8a71ec\n",
            "  Building wheel for objax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for objax: filename=objax-1.6.0-py3-none-any.whl size=86413 sha256=2badc3ad3fc36055abeb6a78f1ea55d3c7d4166c1e10ebf4c857f41beec1da08\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/ea/55/0e523ada24fca022896e3dabb23bc94cc56b00d97461dc1978\n",
            "  Building wheel for ratelimit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ratelimit: filename=ratelimit-2.2.1-py3-none-any.whl size=5908 sha256=4bf6ed3543801ee6ef2e3b16f413e63a4dc15a4ce86bf1623100150a8305b3b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/1e/97/126009a0884bdf7e26436cace73d9a4f4596dada4fdc4950ce\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=7a5e1ad032770d66d5798e9528e12f41b8be7a9779af11b89df9b8e99adb09cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/a5/92/2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2955 sha256=006f1183bd87442b88f91f280f6c098cd0fe2cb1e1866d6cdd3b610e9aba4743\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/e0/3d/9d0c2020c44a519b9f02ab4fa6d2a4a996c98d79ab2f569fa1\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19579 sha256=e5c8407453a0a70ed4eafc974f1766fc2d3788f3c11ad49c5ee08c14b61e4260\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/f0/a8/1094fca7a7e5d0d12ff56e0c64675d72aa5cc81a5fc200e849\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=603b149915bd5a0a5e63633f8f0a1d3e5c1a13ee87aadb31edcbff215b7ca05a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built mordred objax ratelimit seqeval sklearn validators pathtools\n",
            "Installing collected packages: tokenizers, sklearn, sentencepiece, ratelimit, pathtools, commonmark, xxhash, wurlitzer, websocket-client, watchdog, validators, uc-micro-py, torch, tensorflow-hub, sniffio, smmap, setproctitle, sentry-sdk, semver, selfies, rich, rfc3986-validator, rfc3339-validator, rdkit, pyzmq, python-json-logger, pympler, parameterized, packaging, networkx, mypy-extensions, multidict, mrcfile, mmtf-python, latexcodec, jmp, jedi, gsd, frozenlist, fasteners, docker-pycreds, dill, colorama, blinker, biopython, beartype, attrs, async-timeout, yarl, typing-inspect, sphinx, responses, pystow, pydeck, pybtex, plum-dispatch, opt-einsum-fx, multiprocess, mordred, marshmallow, markdown-it-py, linkify-it-py, jupyter-server-terminals, jupyter-client, jsonschema, huggingface-hub, GridDataFormats, gitdb, dm-haiku, anyio, aiosignal, transformers, sphinx_togglebutton, sphinx-thebe, sphinx-multitoc-numbering, sphinx-jupyterbook-latex, sphinx-external-toc, sphinx-design, sphinx-copybutton, sphinx-comments, skunk, seqeval, pydata-sphinx-theme, pybtex-docutils, mdit-py-plugins, MDAnalysis, marshmallow-enum, GitPython, e3nn, aiohttp, wandb, synspace, streamlit, sphinxcontrib-bibtex, sphinx_book_theme, nbclient, myst-parser, jupyter-events, dataclasses-json, objax, langchain, datasets, tensorflow-decision-forests, simpletransformers, jupyter-server, exmol, emlp, tensorflowjs, jupyter-server-mathjax, nbdime, jupyter-sphinx, jupyter-cache, myst-nb, jupyter-book, dmol-book\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: tensorflow-hub\n",
            "    Found existing installation: tensorflow-hub 0.13.0\n",
            "    Uninstalling tensorflow-hub-0.13.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.13.0\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.3.2\n",
            "    Uninstalling rich-13.3.2:\n",
            "      Successfully uninstalled rich-13.3.2\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 23.2.1\n",
            "    Uninstalling pyzmq-23.2.1:\n",
            "      Successfully uninstalled pyzmq-23.2.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.0\n",
            "    Uninstalling packaging-23.0:\n",
            "      Successfully uninstalled packaging-23.0\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.0\n",
            "    Uninstalling networkx-3.0:\n",
            "      Successfully uninstalled networkx-3.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 22.2.0\n",
            "    Uninstalling attrs-22.2.0:\n",
            "      Successfully uninstalled attrs-22.2.0\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 3.5.4\n",
            "    Uninstalling Sphinx-3.5.4:\n",
            "      Successfully uninstalled Sphinx-3.5.4\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 2.2.0\n",
            "    Uninstalling markdown-it-py-2.2.0:\n",
            "      Successfully uninstalled markdown-it-py-2.2.0\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.3.3\n",
            "    Uninstalling jsonschema-4.3.3:\n",
            "      Successfully uninstalled jsonschema-4.3.3\n",
            "  Attempting uninstall: nbclient\n",
            "    Found existing installation: nbclient 0.7.2\n",
            "    Uninstalling nbclient-0.7.2:\n",
            "      Successfully uninstalled nbclient-0.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.12.0 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.12.0 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.12.0 which is incompatible.\n",
            "statsmodels 0.13.5 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.31 GridDataFormats-1.0.1 MDAnalysis-2.4.2 aiohttp-3.8.4 aiosignal-1.3.1 anyio-3.6.2 async-timeout-4.0.2 attrs-21.4.0 beartype-0.12.0 biopython-1.81 blinker-1.5 colorama-0.4.6 commonmark-0.9.1 dataclasses-json-0.5.7 datasets-2.10.1 dill-0.3.6 dm-haiku-0.0.9 dmol-book-1.3.2 docker-pycreds-0.4.0 e3nn-0.5.1 emlp-1.0.2 exmol-3.0.2 fasteners-0.18 frozenlist-1.3.3 gitdb-4.0.10 gsd-2.8.0 huggingface-hub-0.13.3 jedi-0.18.2 jmp-0.0.4 jsonschema-3.2.0 jupyter-book-0.13.1 jupyter-cache-0.4.3 jupyter-client-8.1.0 jupyter-events-0.6.3 jupyter-server-2.5.0 jupyter-server-mathjax-0.2.6 jupyter-server-terminals-0.4.4 jupyter-sphinx-0.3.2 langchain-0.0.123 latexcodec-2.0.1 linkify-it-py-1.0.3 markdown-it-py-1.1.0 marshmallow-3.19.0 marshmallow-enum-1.5.1 mdit-py-plugins-0.2.8 mmtf-python-1.1.3 mordred-1.2.0 mrcfile-1.4.3 multidict-6.0.4 multiprocess-0.70.14 mypy-extensions-1.0.0 myst-nb-0.13.2 myst-parser-0.15.2 nbclient-0.5.13 nbdime-3.1.1 networkx-2.8.8 objax-1.6.0 opt-einsum-fx-0.1.4 packaging-20.9 parameterized-0.9.0 pathtools-0.1.2 plum-dispatch-2.0.1 pybtex-0.24.0 pybtex-docutils-1.0.2 pydata-sphinx-theme-0.8.1 pydeck-0.8.0 pympler-1.0.1 pystow-0.5.0 python-json-logger-2.0.7 pyzmq-25.0.2 ratelimit-2.2.1 rdkit-2022.9.5 responses-0.18.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rich-13.1.0 selfies-2.1.1 semver-2.13.0 sentencepiece-0.1.97 sentry-sdk-1.18.0 seqeval-1.2.2 setproctitle-1.3.2 simpletransformers-0.63.9 sklearn-0.0.post1 skunk-1.2.0 smmap-5.0.0 sniffio-1.3.0 sphinx-4.5.0 sphinx-comments-0.0.3 sphinx-copybutton-0.5.1 sphinx-design-0.1.0 sphinx-external-toc-0.2.4 sphinx-jupyterbook-latex-0.4.7 sphinx-multitoc-numbering-0.1.3 sphinx-thebe-0.1.2 sphinx_book_theme-0.3.3 sphinx_togglebutton-0.3.2 sphinxcontrib-bibtex-2.5.0 streamlit-1.20.0 synspace-0.2.0 tensorflow-decision-forests-1.2.0 tensorflow-hub-0.12.0 tensorflowjs-4.3.0 tokenizers-0.13.2 torch-1.12.0 transformers-4.27.3 typing-inspect-0.8.0 uc-micro-py-1.0.1 validators-0.20.0 wandb-0.14.0 watchdog-3.0.0 websocket-client-1.5.1 wurlitzer-3.0.3 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t8lIsydcXQm"
      },
      "source": [
        "### Load Data\n",
        "\n",
        "Download the data and load it into a [Pandas](https://pandas.pydata.org/) data frame. The hidden cells below sets-up our imports and/or install necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "hide-cell"
        ],
        "id": "HnFb4qjDcXQn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from jax.example_libraries import optimizers\n",
        "import sklearn.manifold, sklearn.cluster\n",
        "import rdkit, rdkit.Chem, rdkit.Chem.Draw\n",
        "import dmol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHPoH7pXcXQo"
      },
      "outputs": [],
      "source": [
        "# soldata = pd.read_csv('https://dataverse.harvard.edu/api/access/datafile/3407241?format=original&gbrecs=true')\n",
        "# had to rehost because dataverse isn't reliable\n",
        "soldata = pd.read_csv(\n",
        "    \"https://github.com/whitead/dmol-book/raw/main/data/curated-solubility-dataset.csv\"\n",
        ")\n",
        "soldata.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiA_2-_ecXQp"
      },
      "source": [
        "### Data Exploration\n",
        "\n",
        "```{margin} EDA\n",
        "If doing EDA as a way to choose features, you should do the train/test/(valid) split prior to EDA to avoid\n",
        "contaminating model selection with test data.\n",
        "```\n",
        "\n",
        "We can see that there are a number of features like molecular weight, rotatable bonds, valence electrons, etc. And of course, there is the label **solubility**. One of the first things we should always do is get familiar with our data in a process that is sometimes called **exploratory data analysis** (EDA). Let's start by examining a few specific examples to get a sense of the range of labels/data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g0OKmazcXQp"
      },
      "outputs": [],
      "source": [
        "# plot one molecule\n",
        "mol = rdkit.Chem.MolFromInchi(soldata.InChI[0])\n",
        "mol"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkhE-D9zcXQp"
      },
      "source": [
        "This is first molecule in the dataset rendered using [rdkit](https://rdkit.org/).\n",
        "\n",
        "Let's now look at the extreme values to get a sense of the **range** of solubility data and the molecules that make it. First, we'll histogram (using {obj}`seaborn.distplot`) the solubility which tells us about the shape of its probability distribution and the extreme values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnx8QChBcXQq"
      },
      "outputs": [],
      "source": [
        "sns.distplot(soldata.Solubility)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyrSO1MacXQq"
      },
      "source": [
        "Above we can see the histogram of the solubility with kernel density estimate overlaid. The histogram shows that the solubility varies from about -13 to 2.5 and is not normally distributed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0xXy3GMcXQq"
      },
      "outputs": [],
      "source": [
        "# get 3 lowest and 3 highest solubilities\n",
        "soldata_sorted = soldata.sort_values(\"Solubility\")\n",
        "extremes = pd.concat([soldata_sorted[:3], soldata_sorted[-3:]])\n",
        "\n",
        "# We need to have a list of strings for legends\n",
        "legend_text = [\n",
        "    f\"{x.ID}: solubility = {x.Solubility:.2f}\" for x in extremes.itertuples()\n",
        "]\n",
        "\n",
        "# now plot them on a grid\n",
        "extreme_mols = [rdkit.Chem.MolFromInchi(inchi) for inchi in extremes.InChI]\n",
        "rdkit.Chem.Draw.MolsToGridImage(\n",
        "    extreme_mols, molsPerRow=3, subImgSize=(250, 250), legends=legend_text\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbImi3gacXQr"
      },
      "source": [
        "The figure of extreme molecules shows highly-chlorinated compounds have the lowest solubility and ionic compounds have higher solubility. Is A-2918 an **outlier**, a mistake? Also, is NH$_3$ really comparable to these organic compounds? These are the kind of questions that you should consider *before* doing any modeling.\n",
        "\n",
        "```{margin} Outliers\n",
        "\n",
        "Outliers are extreme values that fall outside of your normal data distribution. They can be mistakes or be from a different distribution (e.g., metals instead of organic molecules). Outliers can have a strong effect on model training.\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s25Cm2pccXQr"
      },
      "source": [
        "### Feature Correlation\n",
        "Now let's examine the features and see how correlated they are with solubility. Note that there are a few columns unrelated to features or solubility: `SD` (standard deviation), `Ocurrences` (how often the molecule occurred in the constituent databases), and `Group` (where the data came from)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvyx0k2BcXQr"
      },
      "outputs": [],
      "source": [
        "features_start_at = list(soldata.columns).index(\"MolWt\")\n",
        "feature_names = soldata.columns[features_start_at:]\n",
        "\n",
        "fig, axs = plt.subplots(nrows=5, ncols=4, sharey=True, figsize=(12, 8), dpi=300)\n",
        "axs = axs.flatten()  # so we don't have to slice by row and column\n",
        "for i, n in enumerate(feature_names):\n",
        "    ax = axs[i]\n",
        "    ax.scatter(\n",
        "        soldata[n], soldata.Solubility, s=6, alpha=0.4, color=f\"C{i}\"\n",
        "    )  # add some color\n",
        "    if i % 4 == 0:\n",
        "        ax.set_ylabel(\"Solubility\")\n",
        "    ax.set_xlabel(n)\n",
        "# hide empty subplots\n",
        "for i in range(len(feature_names), len(axs)):\n",
        "    fig.delaxes(axs[i])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY-gdSFFcXQr"
      },
      "source": [
        "It's interesting that molecular weight or hydrogen bond numbers seem to have little correlation, at least from this plot. MolLogP, which is a calculated descriptor related to solubility, does correlate well. You can also see that some of these features have low **variance**, meaning the value of the feature changes little or not at all for many data points (e.g., \"NumHDonors\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct3lEATUcXQr"
      },
      "source": [
        "### Linear Model\n",
        "\n",
        "Let's begin with one of the simplest approaches — a linear model. This is our first type of supervised learning and is rarely used due to something we'll see — the difficult choice of features. \n",
        "\n",
        "\n",
        "```{margin} Autodiff\n",
        "[Autodiff](https://en.wikipedia.org/wiki/Automatic_differentiation) is a computer program tool\n",
        "that can compute analytical gradients with respect to two variables in a program. \n",
        "```\n",
        "\n",
        "Our model will be defined by this equation:\n",
        "\n",
        "\\begin{equation}\n",
        "    y = \\vec{w} \\cdot \\vec{x} + b\n",
        "\\end{equation}\n",
        "\n",
        "which is defined for a single data point. The shape of a single feature vector,  $\\vec{x}$, is 17 in our case (for the 17 features). $\\vec{w}$ is a vector of adjustable parameters of length 17 and $b$ is an adjustable scalar (called **bias**).\n",
        "\n",
        "We'll implement this model using a library called [``jax``](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html) that is very similar to numpy except it can compute analytical gradients easily via autodiff.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgnidQG8cXQs"
      },
      "outputs": [],
      "source": [
        "def linear_model(x, w, b):\n",
        "    return jnp.dot(x, w) + b\n",
        "\n",
        "\n",
        "# test it out\n",
        "x = np.array([1, 0, 2.5])\n",
        "w = np.array([0.2, -0.5, 0.4])\n",
        "b = 4.3\n",
        "\n",
        "linear_model(x, w, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diopjpI5cXQs"
      },
      "source": [
        "```{margin} Loss\n",
        "A loss is a function which takes in a model prediction $\\hat{y}$,\n",
        "labels $y$, and computes a scalar representing how poor the fit is.\n",
        "Our goal is to minimize loss.\n",
        "```\n",
        "\n",
        "Now comes the critical question: *How do we find the adjustable parameters $\\vec{w}$ and $b$*? The classic solution for linear regression is computing the adjustable parameters directly with a pseudo-inverse, $\\vec{w} = (X^TX)^{-1}X^{T}\\vec{y}$. You can read more about [this here](https://nbviewer.jupyter.org/github/whitead/numerical_stats/blob/master/unit_12/lectures/lecture_1.ipynb#Extending-Least-Squares-to-Multiple-Dimensions-in-Domain---OLS-ND). We'll use an **iterative** approach that mirrors what we'll do in deep learning. This is not the correct approach for linear regression, but it'll be useful for us to get used to the iterative approach since we'll see it so often in deep learning. \n",
        "\n",
        "To iteratively find our adjustable parameters, we will pick a **loss** function and minimize with **gradients**. Let's define these quantities and compute our loss with some initial random w and b."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GfU28uJcXQt"
      },
      "outputs": [],
      "source": [
        "# convert data into features, labels\n",
        "features = soldata.loc[:, feature_names].values\n",
        "labels = soldata.Solubility.values\n",
        "\n",
        "feature_dim = features.shape[1]\n",
        "\n",
        "# initialize our paramaters\n",
        "w = np.random.normal(size=feature_dim)\n",
        "b = 0.0\n",
        "\n",
        "\n",
        "# define loss\n",
        "def loss(y, labels):\n",
        "    return jnp.mean((y - labels) ** 2)\n",
        "\n",
        "\n",
        "# test it out\n",
        "y = linear_model(features, w, b)\n",
        "loss(y, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceyJDMNpcXQt"
      },
      "source": [
        "Wow! Our loss is terrible, especially considering that solubilities are between -13 and 2. But, that's right since we just guessed our initial parameters. \n",
        "\n",
        "\n",
        "\n",
        "### Gradient Descent\n",
        "\n",
        "We will now try to reduce loss by using information about how it changes with respect to the adjustable parameters. If we write our loss as:\n",
        "\n",
        "\\begin{equation}\n",
        "    L = \\frac{1}{N}\\sum_i^N \\left[y_i - f(\\vec{x}_i, \\vec{w}, b)\\right]^2\n",
        "\\end{equation}\n",
        "\n",
        "This loss is called **mean squared error**, often abbreviated MSE. We can compute our loss gradients with respect to the adjustable parameters:\n",
        "\n",
        "```{margin} jax.grad\n",
        "[jax.grad](https://jax.readthedocs.io/en/latest/jax.html#jax.grad) computes an analytical derivative of a Python function. \n",
        "It takes two arguments: the function and which args to \n",
        "take the derivative of. For example, consider `f(x, y, z)`, then `jax.grad(f,(1,2))`\n",
        "gives $\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}$. Note too that\n",
        "$x$ may be a tensor. \n",
        "```\n",
        "\n",
        "\\begin{equation}\n",
        "    \\frac{\\partial L}{\\partial w_i}, \\frac{\\partial L}{\\partial b}\n",
        "\\end{equation}\n",
        "\n",
        "where $w_i$ is the $i$th element of the weight vector $\\vec{w}$. We can reduce the loss by taking a step in the direction of its negative gradient:\n",
        "\\begin{equation}\n",
        "    (w_i, b') = \\left(w_i - \\eta \\frac{\\partial L}{\\partial w_i}, b - \\eta\\frac{\\partial L}{\\partial b}\\right)\n",
        "\\end{equation}\n",
        "\n",
        "where $\\eta$ is **learning rate**, which an adjustable but not trained parameter (an example of a **hyperparameter**) which we just guess to be $1\\times10^{-6}$ in this example. Typically, it's chosen to be some power of 10 that is at most 0.1. Values higher than that cause stability problems. Let's try this procedure, which is called **gradient descent**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOHzzTiGcXQu"
      },
      "outputs": [],
      "source": [
        "# compute gradients\n",
        "def loss_wrapper(w, b, data):\n",
        "    features = data[0]\n",
        "    labels = data[1]\n",
        "    y = linear_model(features, w, b)\n",
        "    return loss(y, labels)\n",
        "\n",
        "\n",
        "loss_grad = jax.grad(loss_wrapper, (0, 1))\n",
        "\n",
        "# test it out\n",
        "loss_grad(w, b, (features, labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eImeR8F0cXQu"
      },
      "source": [
        "We've computed the gradient. Now we'll minimize it over a few steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW38zN5lcXQu"
      },
      "outputs": [],
      "source": [
        "loss_progress = []\n",
        "eta = 1e-6\n",
        "data = (features, labels)\n",
        "for i in range(10):\n",
        "    grad = loss_grad(w, b, data)\n",
        "    w -= eta * grad[0]\n",
        "    b -= eta * grad[1]\n",
        "    loss_progress.append(loss_wrapper(w, b, data))\n",
        "plt.plot(loss_progress)\n",
        "\n",
        "plt.xlabel(\"Step\")\n",
        "plt.yscale(\"log\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Full Dataset Training Curve\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StfF0OrZcXQu"
      },
      "source": [
        "### Training Curve\n",
        "\n",
        "The figure above is called a **training curve**. We'll see these frequently in this book and they show us if the loss is decreasing, indicating the model is learning. Training curves are also called **learning curves**. The x-axis may be example number, total iterations through dataset (called epochs), or some other measure of amount of data used for training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYFeqPSRcXQv"
      },
      "source": [
        "### Batching\n",
        "\n",
        "```{margin} batch\n",
        "A batch is a subset of your data of size *batch size*. Batch size is usually as a power of 2 (e.g., 16, 128).\n",
        "Having random batches of data is how gradient descent becomes stochastic gradient descent.\n",
        "```\n",
        "\n",
        "This is making good progress. But let's try to speed things up with a small change. We'll use **batching**, which is how training is actually done in machine learning.  The small change is that rather than using all data at once, we only take a small **batch** of data. Batching provides two benefits: it reduces the amount of time to compute an update to our parameters, and it makes the training process random. The randomness makes it possible to escape local minima that might stop training progress. This addition of batching makes our algorithm **stochastic** and thus we call this procedure **stochastic gradient descent** (SGD). SGD, and variations of it, are the most common methods of training in deep learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs360Wk4cXQv"
      },
      "outputs": [],
      "source": [
        "# initialize our paramaters\n",
        "# to be fair to previous method\n",
        "w = np.random.normal(size=feature_dim)\n",
        "b = 0.0\n",
        "\n",
        "loss_progress = []\n",
        "eta = 1e-6\n",
        "batch_size = 32\n",
        "N = len(labels)  # number of data points\n",
        "data = (features, labels)\n",
        "# compute how much data fits nicely into a batch\n",
        "# and drop extra data\n",
        "new_N = len(labels) // batch_size * batch_size\n",
        "\n",
        "# the -1 means that numpy will compute\n",
        "# what that dimension should be\n",
        "batched_features = features[:new_N].reshape((-1, batch_size, feature_dim))\n",
        "batched_labels = labels[:new_N].reshape((-1, batch_size))\n",
        "# to make it random, we'll iterate over the batches randomly\n",
        "indices = np.arange(new_N // batch_size)\n",
        "np.random.shuffle(indices)\n",
        "for i in indices:\n",
        "    # choose a random set of\n",
        "    # indices to slice our data\n",
        "    grad = loss_grad(w, b, (batched_features[i], batched_labels[i]))\n",
        "    w -= eta * grad[0]\n",
        "    b -= eta * grad[1]\n",
        "    # we still compute loss on whole dataset, but not every step\n",
        "    if i % 10 == 0:\n",
        "        loss_progress.append(loss_wrapper(w, b, data))\n",
        "\n",
        "plt.plot(np.arange(len(loss_progress)) * 10, loss_progress)\n",
        "plt.xlabel(\"Step\")\n",
        "plt.yscale(\"log\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Batched Loss Curve\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbBvaPmscXQv"
      },
      "source": [
        "There are three changes to note:\n",
        "\n",
        "1. The loss is lower than without batching\n",
        "2. There are more steps, even though we iterated over our dataset once instead of 10 times\n",
        "3. The loss doesn't always go down\n",
        "\n",
        "The reason the loss is lower is because we're able to take more steps even though we only see each data point once. That's because we update at each batch, giving more updates per iteration over the dataset. Specifically if $B$ is batch size, there are $N / B$ updates for every 1 update in the original gradient descent. The reason the loss doesn't always go down is that each time we evaluate it, it's on a different set of data. Some molecules are harder to predict than others. Also, each step we take in minimizing loss may not be correct because we only updated our parameters based on one batch. Assuming our batches are mixed though, we will always improve in expectation (on average). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqIG84yvcXQv"
      },
      "source": [
        "### Standardize features\n",
        "\n",
        "It seems we cannot get past a certain loss. If you examine the gradients you'll see some of them are very large and some are very small. Each of the features have different magnitudes. For example, molecular weights are large numbers. The number of rings in a molecule is a small number. Each of these must use the same learning rate, $\\eta$, and that is ok for some but too small for others. If we increase $\\eta$, our training procedure will explode because of these larger feature gradients. A standard trick we can do is make all the features have the same magnitude, using the equation for standardization you might see in your statistics textbook:\n",
        "\n",
        "\\begin{equation}\n",
        "    x_{ij} = \\frac{x_{ij} - \\bar{x_j}}{\\sigma_{x_j}}\n",
        "\\end{equation}\n",
        "\n",
        "where $\\bar{x_j}$ is column mean and $\\sigma_{x_j}$ is column standard deviation. To be careful about contaminating training data with test data -- leaking information between train and test data -- we should only use training data in computing the mean and standard deviation. We want our test data to approximate how we'll use our model on unseen data, so we cannot know what these unseen features means/standard deviations might be and thus cannot use them at training time for standardization. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J1SV_hFcXQv"
      },
      "outputs": [],
      "source": [
        "fstd = np.std(features, axis=0)\n",
        "fmean = np.mean(features, axis=0)\n",
        "std_features = (features - fmean) / fstd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZCSVwG_cXQv"
      },
      "outputs": [],
      "source": [
        "# initialize our paramaters\n",
        "# since we're changing the features\n",
        "w = np.random.normal(scale=0.1, size=feature_dim)\n",
        "b = 0.0\n",
        "\n",
        "\n",
        "loss_progress = []\n",
        "eta = 1e-2\n",
        "batch_size = 32\n",
        "N = len(labels)  # number of data points\n",
        "data = (std_features, labels)\n",
        "# compute how much data fits nicely into a batch\n",
        "# and drop extra data\n",
        "new_N = len(labels) // batch_size * batch_size\n",
        "num_epochs = 3\n",
        "\n",
        "# the -1 means that numpy will compute\n",
        "# what that dimension should be\n",
        "batched_features = std_features[:new_N].reshape((-1, batch_size, feature_dim))\n",
        "batched_labels = labels[:new_N].reshape((-1, batch_size))\n",
        "indices = np.arange(new_N // batch_size)\n",
        "\n",
        "# iterate through the dataset 3 times\n",
        "for epoch in range(num_epochs):\n",
        "    # to make it random, we'll iterate over the batches randomly\n",
        "    np.random.shuffle(indices)\n",
        "    for i in indices:\n",
        "        # choose a random set of\n",
        "        # indices to slice our data\n",
        "        grad = loss_grad(w, b, (batched_features[i], batched_labels[i]))\n",
        "        w -= eta * grad[0]\n",
        "        b -= eta * grad[1]\n",
        "        # we still compute loss on whole dataset, but not every step\n",
        "        if i % 50 == 0:\n",
        "            loss_progress.append(loss_wrapper(w, b, data))\n",
        "\n",
        "plt.plot(np.arange(len(loss_progress)) * 50, loss_progress)\n",
        "plt.xlabel(\"Step\")\n",
        "plt.yscale(\"log\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-CYuE0GcXQw"
      },
      "source": [
        "Notice we safely increased our learning rate to 0.01, which is possible because all the features are of similar magnitude. We also could keep training, since we're gaining improvements. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVcLZMiscXQw"
      },
      "source": [
        "### Analyzing Model Performance\n",
        "\n",
        "This is a large topic that we'll explore more, but the first thing we typically examine in supervised learning is a **parity plot**, which shows our predictions vs. our label prediction. What's nice about this plot is that it works no matter what the dimensions of the features are. A perfect fit would fall onto the line at $y = \\hat{y}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4LfTqvwcXQw"
      },
      "outputs": [],
      "source": [
        "predicted_labels = linear_model(std_features, w, b)\n",
        "\n",
        "plt.plot([-100, 100], [-100, 100])\n",
        "plt.scatter(labels, predicted_labels, s=4, alpha=0.7)\n",
        "plt.xlabel(\"Measured Solubility $y$\")\n",
        "plt.ylabel(\"Predicted Solubility $\\hat{y}$\")\n",
        "plt.xlim(-13.5, 2)\n",
        "plt.ylim(-13.5, 2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKfqETz8cXQw"
      },
      "source": [
        "Final model assessment can be done with loss, but typically other metrics are also used. In regression, a **correlation coefficient** is typically reported in addition to loss. In our example, this is computed as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD-REFjncXQx"
      },
      "outputs": [],
      "source": [
        "# slice correlation between predict/labels\n",
        "# from correlation matrix\n",
        "np.corrcoef(labels, predicted_labels)[0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "remove-cell"
        ],
        "id": "wEF3aaD9cXQx"
      },
      "outputs": [],
      "source": [
        "# THIS CELL IS USED TO GENERATE A FIGURE\n",
        "# AND NOT RELATED TO CHAPTER\n",
        "# YOU CAN SKIP IT\n",
        "from myst_nb import glue\n",
        "\n",
        "glue(\"corr\", np.round(np.corrcoef(labels, predicted_labels)[0, 1], 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQimAACwcXQx"
      },
      "source": [
        "A correlation coefficient of {glue:}`corr` is OK, but not great."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVUdiq8RcXQx"
      },
      "source": [
        "## Unsupervised Learning\n",
        "\n",
        "In unsupervised learning, the goal is to predict $\\hat{y}$ *without* labels. This seems like an impossible task. How do we judge success? Typically, unsupervised learning can be broken into three categories:\n",
        "\n",
        "**Clustering**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Here we assume $\\{y_i\\}$ is a class variable and try to partition our features into these classes. In clustering we are simultaneously learning the definition of the classes (called clusters) and which cluster each feature should be assigned to.\n",
        "\n",
        "```{margin} Class\n",
        "In machine learning, a class is a type of label like ``dog`` or ``cat``. Formally,\n",
        "we have a set of possible labels (e.g., all animals) and each feature vector has one (hard) or a \n",
        "probability distribution of classes (soft).\n",
        "```\n",
        "\n",
        "**Finding Signal**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $x$ is assumed to be made of two components: noise and signal ($y$). We try to separate the signal $y$ from $x$ and discard noise. Highly-related with **representation learning**, which we'll see later.\n",
        "\n",
        "\n",
        "**Generative**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Generative methods are methods where we try to learn $P(\\vec{x})$ so that we can sample new values of $\\vec{x}$. It is analogous to $y$ being probability and we're trying to estimate it. We'll see these more later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE05QCyKcXQx"
      },
      "source": [
        "### Clustering\n",
        "\n",
        "Clustering is historically one of the most well-known and still popular machine learning methods. It's always popular because it can provide new insight from data. Clustering gives class labels where none existed and thus can help find patterns in data. This is also a reason that it has become less popular in chemistry (and most fields): there is no right or wrong answer. Two people doing clustering independently will often arrive at different answers. Nevertheless, it should be a tool you know and can be a good exploration strategy.\n",
        "\n",
        "```{margin} cluster labels\n",
        "Clustering comes in many variants and some blur what exactly $y_i$ is. For example, in some clustering methods $y_i$ can include no assignment or $y_i$ is not a single class, but a tree of classes.\n",
        "```\n",
        "\n",
        "We'll look at the classic clustering method: k-means. Wikipedia has a [great article](https://en.wikipedia.org/wiki/K-means_clustering) on this classic algorithm, so I won't try to repeat that. To make our clustering actually visible, we'll start by projecting our features into 2 dimensions. This will be covered in representation learning, so don't worry about these steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36jzUL2FcXQx"
      },
      "outputs": [],
      "source": [
        "# get down to 2 dimensions for easy visuals\n",
        "embedding = sklearn.manifold.Isomap(n_components=2)\n",
        "# only fit to every 25th point to make it fast\n",
        "embedding.fit(std_features[::25, :])\n",
        "reduced_features = embedding.transform(std_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGnsIy2ycXQx"
      },
      "source": [
        "We're going to zoom into the middle 99th percentile of the data since some of the points are extremely far away (though that is interesting!). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UziKTSnhcXQy"
      },
      "outputs": [],
      "source": [
        "xlow, xhi = np.quantile(reduced_features, [0.005, 0.995], axis=0)\n",
        "\n",
        "plt.figure(dpi=300)\n",
        "plt.scatter(\n",
        "    reduced_features[:, 0],\n",
        "    reduced_features[:, 1],\n",
        "    s=4,\n",
        "    alpha=0.7,\n",
        "    c=labels,\n",
        "    edgecolors=\"none\",\n",
        ")\n",
        "plt.xlim(xlow[0], xhi[0])\n",
        "plt.ylim(xlow[1], xhi[1])\n",
        "cb = plt.colorbar()\n",
        "cb.set_label(\"Solubility\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EAalI0VcXQy"
      },
      "source": [
        "```{margin} Dimensionality Reduction\n",
        "Reducing $\\vec{x}$, your feature vectors to a low\n",
        "dimensional space. The classic example is PCA, which is a \n",
        "linear operator. However, most prefer nonlinear methods \n",
        "like [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "The dimensionality reduction has made our features only 2 dimensions. We can see some structure, especially with the solubility as the coloring. Note in these kind of plots, where we have reduced dimensions in someway, we do not label the axes because they are arbitrary.\n",
        "\n",
        "Now we cluster. The main challenge in clustering is deciding how many clusters there should be. There are a number of methods out there, but they basically come down to intuition. You, as the chemist, should use some knowledge outside of the data to intuit what is the cluster number. Sounds unscientific? Yeah, that's why clustering is hard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "remove-output"
        ],
        "id": "nE0xJSQ7cXQy"
      },
      "outputs": [],
      "source": [
        "# cluster - using whole features\n",
        "kmeans = sklearn.cluster.KMeans(n_clusters=4, random_state=0)\n",
        "kmeans.fit(std_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WcxgqndcXQy"
      },
      "source": [
        "Very simple procedure! Now we'll visualize by coloring our data by the class assigned. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO2YzuYOcXQy"
      },
      "outputs": [],
      "source": [
        "plt.figure(dpi=300)\n",
        "point_colors = [f\"C{i}\" for i in kmeans.labels_]\n",
        "plt.scatter(\n",
        "    reduced_features[:, 0],\n",
        "    reduced_features[:, 1],\n",
        "    s=4,\n",
        "    alpha=0.7,\n",
        "    c=point_colors,\n",
        "    edgecolors=\"none\",\n",
        ")\n",
        "# make legend\n",
        "legend_elements = [\n",
        "    plt.matplotlib.patches.Patch(\n",
        "        facecolor=f\"C{i}\", edgecolor=\"none\", label=f\"Class {i}\"\n",
        "    )\n",
        "    for i in range(4)\n",
        "]\n",
        "plt.legend(handles=legend_elements)\n",
        "plt.xlim(xlow[0], xhi[0])\n",
        "plt.ylim(xlow[1], xhi[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbol6FECcXQy"
      },
      "source": [
        "### Choosing Cluster Number\n",
        "\n",
        "How do we know we had the correct number? Intuition. There is one tool we can use to help us, called an **elbow plot**. The k-means clusters can be used to compute the mean squared distance from cluster center, basically a version of loss function. However, if we treat cluster number as a trainable parameter we'd find the best fit at the cluster number being equal to number of data points. Not helpful! However, we can see when the slope of this loss becomes approximately constant and assume that those extra clusters are adding no new insight. Let's plot the loss and see what happens. Note we'll be using a subsample of the dataset to save time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S76F40e1cXQy"
      },
      "outputs": [],
      "source": [
        "# make an elbow plot\n",
        "loss = []\n",
        "cn = range(2, 15)\n",
        "for i in cn:\n",
        "    kmeans = sklearn.cluster.KMeans(n_clusters=i, random_state=0)\n",
        "    # use every 50th point\n",
        "    kmeans.fit(std_features[::50])\n",
        "    # we get score -> opposite of loss\n",
        "    # so take -\n",
        "    loss.append(-kmeans.score(std_features[::50]))\n",
        "\n",
        "plt.plot(cn, loss, \"o-\")\n",
        "plt.xlabel(\"Cluster Number\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Elbow Plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqA20Z_xcXQz"
      },
      "source": [
        "Where is the transition? If I squint, maybe at 6? 3? 4? 7? Let's choose 4 because it sounds nice and is plausible based on the data. The last task is to get some insight into what the clusters actually are. We can extract the most centered data points (closest to cluster center) and consider them to be representative of the cluster. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "alt": "Grid of rendered molecular structures that are representative cluster centers",
        "id": "X6QTChChcXQz"
      },
      "outputs": [],
      "source": [
        "# cluster - using whole features\n",
        "kmeans = sklearn.cluster.KMeans(n_clusters=4, random_state=0)\n",
        "kmeans.fit(std_features)\n",
        "\n",
        "cluster_center_idx = []\n",
        "for c in kmeans.cluster_centers_:\n",
        "    # find point closest\n",
        "    i = np.argmin(np.sum((std_features - c) ** 2, axis=1))\n",
        "    cluster_center_idx.append(i)\n",
        "cluster_centers = soldata.iloc[cluster_center_idx, :]\n",
        "\n",
        "legend_text = [f\"Class {i}\" for i in range(4)]\n",
        "\n",
        "# now plot them on a grid\n",
        "cluster_mols = [rdkit.Chem.MolFromInchi(inchi) for inchi in cluster_centers.InChI]\n",
        "rdkit.Chem.Draw.MolsToGridImage(\n",
        "    cluster_mols, molsPerRow=2, subImgSize=(400, 400), legends=legend_text\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLyXVr5AcXQz"
      },
      "source": [
        "So what exactly are these classes? Unclear. We intentionally did not reveal solubility (unsupervised learning) so there is not necessarily any connection with solubility. These classes are more a result of which features were chosen for the dataset. You could make a hypothesis, like class 1 is all negatively charged or class 0 is aliphatic, and investigate. Ultimately though there is no *best* clustering and often unsupervised learning is more about finding insight or patterns and not about producing a highly-accurate model.\n",
        "\n",
        "The elbow plot method is one of many approaches to selecting cluster number {cite}`pham2005selection`. I prefer it because it's quite clear that you are using intuition. More sophisticated methods sort-of conceal the fact that there is no right or wrong answer in clustering. \n",
        "\n",
        "\n",
        "\n",
        "```{note}\n",
        "This process does not result in a function that predicts solubility. We might try to gain insight about predicting solubility with our predicted classes, but that is not the goal of clustering.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNQ0rZPGcXQz"
      },
      "source": [
        "## Chapter Summary\n",
        "\n",
        "* Supervised machine learning is building models that can predict labels $y$ from input features $\\vec{x}$.\n",
        "* Data can be labeled or unlabeled. \n",
        "* Models are trained by minimizing loss with stochastic gradient descent.\n",
        "* Unsupervised learning is building models that can find patterns in data.\n",
        "* Clustering is unsupervised learning where the model groups the data points into clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-bkO-ymcXQz"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "### Data\n",
        "\n",
        "1. Using `numpy` reductions `np.amin`, `np.std`, etc. (not `pandas`!), compute the mean, min, max, and standard deviation for each feature across all data points. \n",
        "\n",
        "2. Use rdkit to draw the 2 highest molecular weight molecules. Note they look strange. \n",
        "\n",
        "### Linear Models\n",
        "\n",
        "1. Prove that a nonlinear model like $y = \\vec{w_1} \\cdot \\sin\\left(\\vec{x}\\right) + \\vec{w_2} \\cdot \\vec{x} + b$ could be represented as a linear model.\n",
        "\n",
        "2. Write out the linear model equation in Einstein notation in batched form. **Batched form** means we explicitly have an index indicating batch. For example, the features will be $x_{bi}$  where $b$ indicates the index in the batch and $i$ indicates the feature.\n",
        "\n",
        "\n",
        "### Minimizing Loss\n",
        "\n",
        "1. We standardized the features, but not the labels. Would standardizing the labels affect our choice of learning rate? Prove your answer.\n",
        "\n",
        "2. Implement a loss that is mean absolute error, instead of mean squared error. Compute its gradient using `jax`.\n",
        "\n",
        "2. Using the standardized features, show what effect batch size has on training. Use batch sizes of 1, 8, 32, 256, 1024. Make sure you re-initialize your weights in between each run. Plot the log-loss for each batch size on the same plot. Describe your results.\n",
        "\n",
        "### Clustering\n",
        "\n",
        "1. We say that clustering is a type of unsupervised learning and that it predicts the labels. What exactly are the predicted labels in clustering? Write down what the predicted labels might look like for a few data points.\n",
        "\n",
        "2. In clustering, we predict labels from features. You can still cluster if you have labels, by just pretending they are features. Give two reasons why it would not be a good idea to do clustering in this manner, where we treat the labels as features and try to predict new labels that represent class.\n",
        "\n",
        "3. On the isomap plot (reduced dimension plot), color the points by which group they fall in (G1, G2, etc.). Is there any relationship between this and the clustering?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EGF_eQycXQz"
      },
      "source": [
        "## Cited References\n",
        "\n",
        "```{bibliography}\n",
        ":style: unsrtalpha\n",
        ":filter: docname in docnames\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}